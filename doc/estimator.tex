\section{Estimator} \label{sec:estimator}
Consider the commonly used Kullback-Leibler (KL) divergence between two
distributions $p(x)$ and $g(x)$:
\beq
    \kl{p}{q} = \int p(x)~\log\frac{p(x)}{q(x)}~{\rm d}x. 
\eeq
The KL divergence between the posterior $p = p(\theta \given D, m)$ and prior
$\pi = p(\theta | m)$ is then 
\beq
    \kl{p}{\pi} = \int p(\theta \given D, m)~\log\frac{p(\theta \given D, m)}{p(\theta | m)}~{\rm d}\theta. 
\eeq
If we use Bayes' Theorem to substitute for $p(\theta \given D, m)$ in the
numerator, we can rewrite this as 
\begin{align}
    \kl{p}{\pi} &= \int p(\theta \given D,
    m)~\log\frac{p(D \given \theta, m)}{p(D | m)}~{\rm d}\theta\\ 
        &= -\log p(D\given m) + \int p(\theta \given D, m)~\log p(D \given \theta, m)~{\rm d}\theta. 
\end{align}
The first term of the right hand side is the log evidence and the second term
is the expectation value of the log likelihood under the posterior, $\big\langle
\log \mathcal{L} \big\rangle$. In standard Bayesian analyses in astronomy where
we use MCMC to sample the posterior, $\big\langle \log \mathcal{L} \big\rangle$
can be easily derived with these samples using Monte Carlo integration: 
\beq
\big\langle \log \mathcal{L} \big \rangle \approx \frac{1}{n}
\sum\limits_{i=1}^{n} \log \mathcal{L}(\theta^{(i)})
\eeq
Therefore, if we can estimate the KL divergence between the posterior and prior
we can also estimate the evidence: 
\beq \label{eq:estimator}
\log p(D\given m) = \big\langle \log \mathcal{L} \big\rangle - \kl{p(D \given \theta, m)}{\pi}.
\eeq

To estimate the divergence, we can make use of non-parametric divergence
estimators, which have been applied to Support Distribution Machines and 
used in the machine learning and astronomical literature~\citep[\emph{e.g.}][]{poczos2011,
poczos2012, poczos2012a, xu2013, ntampaka2015, ntampaka2016, ravanbakhsh2017,
hahn2019}. These estimators allow us to estimate the divergence between 
distributions $p$ and $q$, $D(p\,||\,q)$ using samples drawn from them. In 
our case, $p$ and $q$ are the posterior and prior distributions. Again,
standard analyses already sample the posterir distribution; the prior 
distribution is typcially straightforward to sample. 

For the KL divergence, we use the $k$-Nearest Neighbor (NN) estimator presented in
\cite{wang2009}. Let $X_{1:n} = \{ X_1, ... X_n \}$ and $Y_{1:m} = \{ Y_1, ... Y_m \}$
be $n$ and $m$ samples drawn from the $p$ and $q$ $d$-dimensional distributions, respectively. 
Then the divergence between $p$ and $q$ can be estimated as: 
\beq \label{eq:div_est}
\kl{p}{q} \approx \widehat{D_{\rm KL}}(p\,||\,q) = \frac{d}{n}
\sum\limits_{i=1}^n \Big[\log \frac{\nu_{\ell_i}(i)}{\rho_{k_i}(i)} \Big] +
\frac{1}{n} \sum\limits_{i=1}^n\Big[\psi(\ell_i) - \psi(k_i)\Big] + \log
\frac{m}{n-1}. 
\eeq
In the first term, $\rho_k(i)$ denotes the Euclidean distance between $X_i$ and 
the $k^\mathrm{th}$-NN of $X_i$ in sample $\{X_j\}_{i\neq j}$. $\nu_\ell(i)$
denotes the Euclidean distance between $X_i$ and the $\ell^\mathrm{th}$ NN of
$X_i$ in the sample $Y_{1:m}$. In the second term, $\psi$ is the Digamma function: 
$\psi(k) = \Gamma'(k)/\Gamma(k)$.
This term corrects for the estimation bias that comes from nonuniformity of 
the distribution near each sample point and guarantees that the estimator is
asymptotically unbiased. Furthermore, while some $k$-NN estimators require 
chome choice in $k$ and $\ell$~\citep[\emph{e.g.}][]{poczos2012}, the 
\cite{wang2009} estimator adaptively determines $\ell_i$ and $k_i$ as the number 
of samples $X_{1:n}$ and $Y_{1:m}$), respectively, contained in $B(X_i, \epsilon(i))$, 
a Euclidean ball centered at $X_i$ with radius $\epsilon(i)$. 
\beq
\epsilon(i) = \max(\rho(i), \nu(i)) 
\eeq
where 
\begin{align}
    \rho(i) &= \min_{j \neq i} || X_i - X_j|| \\
    \nu(i) &= \min_{j \neq i} || X_i - Y_j||.
\end{align}
For further details on the estimator and proofs that the estimator is
asymptotically unbiased and mean-square consistent we refer readers to
\cite{wang2009}.
